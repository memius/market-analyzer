checkout in tidligere versjon; se at den virker. diff den og
current. se hva forskjellen er, som gjør at den ikke vil displaye
tekst.





txt ut fra fetch.article(link) er ikke "None", men heller ikke tekst,
selv om den skulle være unicode.

finn ut hva den er. kjør f.eks. programmene uten alt web-pisset. bare
kjør sites, fetch, classify, og når det funker, så putter du det inn i
web-servicen.

on the other hand, you won't have the db without the web engine...









content er ikke tom, men fetch.article(link) henter ingenting. dersom
du setter link til a være content, så funker det, så content blir
stored dersom det er noe i den variabelen.


ok, så article.content er faktisk tom. det funker med andre ting (url,
"egen tekst"), så da er det rett og slett tomt i
article.content. hvorfor blir ikke teksten lagret der, da? den blir jo
lagret i url, timestamp.



duplikatsjekk mot siste uke for akkurat det selskapet. en uke holder.

gjør først en duplikatsjekk på linkene i listen du skal søke gjennom;
da slipper du å hente duplikatartikler.

hvordan sjekker man effektivt om to tekster er duplikater?





see if the code runs as it is now. you should get an error. remove the
prediction and 0auth stuff, and get it running without that. it should
be only in main.py. 











get the apple bucket name, get it registered, and train it with an
example.

test it on the same article, and on another article.

get a filter in place that ensures an article will only be stored and
trained once.

make the ui that enables you to train articles manually. show the
text, and have four links: correct|pos|neutral|neg. let the next
article pop up when you click one of these.




make another bucket for google. do the same stuff with that.


get the automation of creating a new bucket in place.

get the access mechanics in place, so each user has his list of picks.

get the buying in place so each user can add to his list.





aaight, so i send all the articles to the same place - the sentiment
is independent of the company, after all. that means i only need to
register one model with google. the form is open in chrome already.

you really should automate genarating a new predictor for each
company, though. sigh. and you should use google, since you want their
smooth scalability. crm would be your own thing, that you would have
to tweak and incorporate and control, i guess.

make the first one apple-specific, and write down every step of the
way to making it work. that way, you know what you have to automate
for the next ones.




i sh-scriptene brukes en google key i stedet for 'language', etc.



no model found. model must first be trained. feil navn på 'language',
da, antakelig.

du må altså finne riktig navn i url'en:

https://www.googleapis.com/prediction/v1.4/trainedmodels/Language%20Identifier/predict?alt=json

Language Identifier, Language Detection, languages, smsspam, languageidentifier og language funker ikke.

etter det kan du bytte den ut med din egen, som du kan kalle news
sentiment e.l. den må trenes før du kan querye den. finn ut hvordan
man gjør det.




det er en error i try credentials, men jeg kan ikke se den, fordi den
printes ikke.

se i predict.py om du har noenlunde samme kall og logikk og gang.

du har byttet ut 'none' med err_str, men det hjalp ikke. ingenting
printes nå.




du må putte tingene fra prediction api inn i templaten, og kalle
templaten etter at du har gjort de tingene.













du får ingen errors, men du får bare displayet 'tmh', selv om du ikke
har slettet noe fra db'en. dvs. at db'en forsvinner hver gang du
stopper serveren? hm. nei, ikke bare kutte serveren. hva da? mye
forandring av kode? tviler litt på det. ny db pga. autorisering, etc?





ok. login required løst ved å importere login_required. det er andre
ting som også må importeres, f.eks. memcache, og kanskje
flowsomething.

nå får du 'tuple not callable' på ('/auth_return',AuthHandler)





try-pred bruker python 27, så det er ikke der problemet ligger.


det funket ikke å inkludere authhandler. hm. det er i alle fall noe
muffens med webapp vs. webapp2. authhandler bruker webapp, og i følge
gae så er det 'no module named webapp'.



name 'login_required' is not defined. sjekk staving, hvor definert, og
hvordan kalt.




auth_return defineres i try-predictions MAIN.PY, som forteller hvilke
klasser som skal handle hvilke url'er. det er authhandler, som nå
ligger nederst i min main.py, som handler auth_return-url'en.

dvs. at så lenge jeg får koblet authhandler på auth-return, så er vi i
boks.







du må ha mange predictors; en for hvert selskap. sjekk pricing.

du må kanskje forandre tittelen til transfarmr? i hvert fall før du
publiserer på nettet. gjør det, og test selv, deretter kan du finne på
et annet navn etterhvert. bruk transfarmr som testbed.



hvis ikke det under funker: kopier try-prediction-scriptet nøyaktig,
og kjør det akkurat slik som beskrevet. se om det funker da, og ekstrapoler.



du trenger antakelig authhandler, som er nederst i home.py. putt den i
main.py, og sjekk at den blir kalt på en eller annen måte.

du må finne ut hva '/auth_return' er for noe, hvor den er definert. se
under: google tror at jeg vil bruke den, i alle fall. den defineres i
if not credentials-seksjonen i main.py. callback = ...



på nettsiden til google, som du havner på når du forsøker på
oauth2client, står det at redirect uri'en er
localhost:8080/auth_return, som du kanskje ikke har definert. har det
noe med dette å gjøre?







i tillegg: hva om scope er for stort? nå ber du jo om en read-write
greie som du ikke har tilgang til, i tillegg til prediction.










du må få definert redirect_url et eller annet sted. det er muligens
det som gjør at auth requesten til google feiler med invalid
client. eller kanskje du har skrevet inn id og secret feil?



du holder på å lese basic steps, på
https://developers.google.com/accounts/docs/OAuth2#scenarios

der vil du få vite hvordan du skal skaffe deg et token.





kanskje du må publishe appen på appengine for at prediction skal
virke? den virker vel neppe lokalt, med mindre du har logget inn som
thomas@dictatr.com, som du kanskje kan gjøre dersom du kommenterer inn
igjen seksjonen om flow i main.py.

-fiks oauth2client credentials

-legg in prediction api. dersom de har fail categories, la neutral
 være det. hvis ikke, la neutral være en egen kategori.

-tren noen artikler.

-se om sentiment classification hjelper deg på noen måte. siden den
 kan bli tatt vekk når som helst må du ikke bli avhengig av den.

-display conclusion om selskap.

-legg inn telleverk som sjekker at antall artikler som blir trent ikke
 er veldig ulikt. f.eks. maks femti artikler flere før du stopper
 treningen.

-legg inn at man bare får sin brukers selskaper.

-legg inn at å legge til et selskap koster penger (dersom man allerede
 har fem eller flere selskaper på listen sin). å legge til selskap
 (til db'en) har du enkel kode for allerede, bare trekk det ut av
 main, og legg det inn i en funksjon.


-legg inn ambiguous articles (display 3 of fewer).

-pynt på displayen av artikler, selskaper. hele siden.

-la magnus, eric, erik evensen, erik aarebrot, thor henning, eskil,
 geir, raymond alfa-teste. fortell dem at de kan kjøpe i vilden sky
 (men tenke at det er ekte penger),
 og at jeg vil kansellere alle innbetalingene deres.

-legg websiden ut på reddit og la folk alfateste den.

-lag app. websiden er gratis, appen koster fem euro. (ellers koster
 ting det samme begge steder.)








drit i adm og optimeringsting; f.eks. hvem som har
tilgang, om ting blir duplisert og om teksten er perfekt.



du får ikke news om selskaper på andre børser enn us. ikke europa, og
ikke asia, f.eks. krx, der samsung er.



bruk url'en til hver artikkel til å sjekke duplikasjon. google
"checking for duplicated content gae put()" or something.



why does it fetch 3 copies of the first article? (3 copies of the
first article link.)



main displays companies from db.

crawl does nothing

sites gives me the links from gf

fetch takes a url (link), reads the article and returns the text

clean cleans the text (called by fetch)

must store junk-cleaned text to db. du er her. let main do this. make
sure the chain from main to sites to fetch and back to main is good.

later, retrieve text from db, clean strict and send to crm. main does
this too.

later, store crm's conclusion to db. and main does this.

later, let main display conclusion with company.



fjern litt javascript i replace characters, ved å fjerne alle ord som
inneholder åpenbare tegn på at de er kode. se fetch.py for detaljer.

lag din egen replace characters. se på gyri sin, og din egen fra
doc/scripts/substitute characters.crm.



kanskje ha en refresh knapp på siden, og bare la ting oppdatere seg
når de kommer, og når de refresher. det er enkelt.




så prøver du med innlogging fra news.google.









har fra news.google uten innlogging. 


linker som ikke er til artikler
fører til krasj.


stripp teksten, og bruk regexer for å finne selve artikklen. beware at
teksten (bloomberg) inneholder instruksjoner og notater til
webutviklerne: "bruk cookie, sett cookien til i morgen."













begynn med en ny side (news.google.com), og prøv å kode slik at det
virker der. deretter tar du yahoo finance. yahoo først, siden google
krever login for å få din egen side med finansnyheter. det er sikkert
lett, gitt at google appengine sin login-funksjon funker. da kan jeg
lese fra min google-konto sin nyhetsside, og legge det inn i db'en.

enkle ad hoc-regex-regler som er greie å begynne med (før den
generelle regex-silingen):
if man finner dette, bruk det, hvis ikke, sil med regex:
dagbladet begynner alltid teksten i artikkelen med "(Dagbladet):"
vg begynner med "(VG nett)"
bloomberg ikke.
dn ikke.
yahoo finance ikke.
ofte står det faktisk (Bloomberg), (Reuters), etc., spesielt dersom
storyen er sakset derfra. Du kan se etter disse.

dersom du skal ha med yahoo finance og google news er det lurt å
begynne med dem, siden de er aggregater av andre nyhetssteder. Dvs. at
det er meningsløst å ta dem med sent, når du allerede har de andre på
plass.




hva med å fjerne alle html-tags(med innhold) som IKKE inneholder minst
ett punktum med space bak (altså minst to setninger)? vha regex,
selvfølgelig, ikke ad hoc.










få til å putte artikkeldelene i db'en.





du har nå lagt inn logging statements i fetch.py som skriver til
fetch.log. det virker ikke med mindre man starter programmet fra kommandolinjen.

nå har du en greie som samler alle artiklene og printer på skjermen,
men den klarer ikke å trekke ut tittel, brødtekst osv. i det hele
tatt.




se på terminaloutputen også når du loader siden (urlfetch bla bla, etc.)









last opp workspace/market_analyzer/ til git.





prøv å bruke en deferred eller asynchronous kommando til å kjøre
nåværende vb.py fra web appen.

så lager du en enkel crawler som kan gi url'er til ovenstående.

så kan du lage en lagrer, som lagrer de innsamlede artiklene (uten
ordentlig siling, ja - du kan jo lagre hver tiende eller noe slikt).





i utgangspunktet funker det å slå sammen vg.py og db.py (i
vb.py). dagbladets titler blir uansett ikke samlet inn for tiden.

et senere steg blir å samle alle selektorene i ett script, og prøve det på
både de norske, og forskjellige internasjonale nettsider.


kan se ut som om shell-appen i eksemplene er en måte å kjøre ting på i
bakgrunnen, uavhengig av brukerne. goodie good!

se også på asynchronous requests (url fetching) du har et vindu åpent
på det.

også background work with the deferred library. vindu åpent på det.

e24 og kapital og de andre pengeavisene (finansavisen, dine penger,
dn.no) har rss feeds.







sjekk hvilke news sites som har api for innsamling av artikler.

sjekk yahoo finance, google news, nyse news, london stock exchange,
forbes, bloomberg, kapital, wall street journal, nyt, techcrunch,
nasdaq, reddit, digg, hacker news, ikke 4chan ennå. hvor har du
skrevet mer om dette? det er flere et sted.

yahoo finance har. nyse også.

twitter har. det finnes også en python-twitter pakke for ubuntu.

nyt har, for bruker-generert innhold. kan sikkert hackes til å virke
for artikler også.

snakk med magnus om opinion-børser, hva de heter, hvordan de
fungerer...



husk at det er data i /home/tmh/django-something i tillegg til det som
er lastet opp til git. disse kan du bruke som grunnlag til å lage nye
admin- og app-templates.

gå gjennom den blå notatboken, samt todo-listen i chrome, todo-listen
på telefonen, 

-lag script som går gjennom prosedyren for å legge til et
 selskap. søketermene manuelt, resten bør være automatisk. først
 prosedyre, på sikt script.

simon i frankrike, magnus, eskil bør være blant alfa-testerne.

chs-filene boostes etter en stunds trening. dette må automatiseres.




DONE:

sjekk status på teab din maskin vs. teab lap og teab på
git. synkroniser, slik at din maskin er oppdatert. done. oppdatert

snakk med eskil. fortell ham at du kutter teab, men starter et
spin-off prosjekt alene. done.

sjekk om lynx og w3m > fil blir bedre enn vb.py. det ble det ikke -
alle linker og denslags er med, uten infoen man får fra
taggene. vanskeligere å parse.

problemet ligger i vb.py. den forårsaker 'illegal seek'. kommenter ut
linjer til du finner hvor feilen ligger. se gjerne gjennom filen
først, for a se om du ser noe åpenbart.

det er bugs i get_Atext, som gjør at teksten fra db ikke kommer med,
og vg gir error - men drit i det. neste steg er som nevnt under å lage
en crawler som samler url'er, og leverer dem til main, som leverer dem
til fetch.py

kan se ut som om du bare trenger en liten kodesnutt som ligner på den
vi brukte i teab til å følge linker. du følger alle hrefs på en side,
og ser hva som skjer. dersom du starter på førstesiden vil du vel til
slutt ha tatt alle linkene på domenet.

se også på hva knut skriver om wget-scriptet sitt.

du har skrevet et script som kjører rdiff-backup, og sender deg en
mail hvis det feiler. det virker antakelig. kjør en gang til og sjekk
at mailen sendes. legg scriptet inn i crontab, både til /media/bck og
til bbg@hjemme.

kjør keygen og ssh copy id begge veier. tri har ekstern ip adresse som
hjemme kan benytte seg av.







du holder på med backups.
finn ut hvordan du gjør rdiff-backup til bbg@hjemme. passord og
slikt. skriv det ned i doc/vikt/rdiff-backup

gjør /bck på hjemme
eid av bbg, og /media/bck eid av tmh.

du fjerner ting fra /bck, og putter dem i /media/bck.

gjør en rdiff-bck fra /home/tmh/ til /media/bck/tmh/ og en fra
/home/bbg/ til /bck/bbg først.

sett locale globalt. det er mulig det hjelper at hele maskinen er klar
for utf-8.

sørg også for å encode til utf-8 før du sender til replace osv.

du har en u på begynnelsen av href-url'ene. kanskje det er den som
lager error?

kanskje den blir fjernet av replace_characters? det er i
alle fall der den burde ligge, tror jeg. encode rett før du sender til
replace, og rett etter at du får ting tilbake derfra.

jeg lurer på om grunnen til at alle gyris scripts har sluttet å virke
er at jeg bruker python 2.5.

i så fall gjør jeg følgende: les på hvor mature python 2.7 er for
google appengine. dersom den er ok, bruk den. hvis ikke, skriv om
beautifulsoup-koden slik at den virker for python 2.5.

bs4 har soup.get_text()!!!!! dersom den virker noenlunde bra er det
himmelsk!

det virker som om python2.7 funker i appengine naa. dessuten blir du
raadet til aa bruke django heller enn webapp templates, which is A
Good Thing.



nå får du tak i brødteksten fra dagbladet vha. editableBodyText
(get_text() er kanskje ikke særlig nyttig likevel). ok,
men teksten forsvinner i løpet av behandlingen av teksten. drit i det,
fokuser på google news med innlogging.

jeg kjører nå python 2.7 og bs4. sjekk om fetch.py funker. putt
deretter inn get_text() og hent news.google.com (i fetch.py) med login
fra mechanize (i crawl.py).

dev_appserver.py får ikke tak i logging-modulen. det er muligens
fordi du har gått over til python 2.7 og ennå ikke har skrevet om
main.py for å reflektere dette. 

drit i github for now. du har lokal git, og du har backup. det holder.

du må sende en post request ved navn 'comp' ('sign') til Companies
handleren for å få lagret ting. dette kan antakelig gjøres enklere
siden du ikke trenger å handle et knappeklikk og tekst i en inputboks.

få main til å vise company name og ticker uten noe annet dill. 

lagre til, og hente fra, db.

du holder på med regex for setninger. du har funnet et par stykker, og
den siste ser ut til å funke på dem som den første ikke funket
på. finn et par til og se.




google.com/finance har relevante nyheter for hver selskap! bruk
det. finn 'a', finn 'lead_story_url:', strengene som kommer etter
det er linker til relevante stories, hi hi.


correct the display of 'user'. this was due to the two debug entries
in fetch.py, where you fetched and printed text from block.py


you have good enough texts. making them better is optimizing.

that way, when they want to read the article, you first show them the
text you put into crm with the link, and if they want to read that,
they click the link. perhaps you can even track that, and look at the
articles for which they click the link, so you can see why they
weren't happy just reading the text for crm.

prøv om wrapperen til crm114 faktisk funker. antakelig ikke, men vær
sikker før du antar!

finn crm.py, laget av openvest development. putt den i folderen, og
rename din egen crm.py til noe annet. se om det da går an å bruke
den. (dersom openvest sin crm.py inneholder subprocess modulen, så vil
det ikke gå. da må du finne på noe annet.







du har problemer med subprocess-modulen, som tilsynelatende ikke har
en call() funksjon, selv om den skal ha det. subprocesses not allowed
in google appengine. d'oh.

how to start crm114 then? some stupid wrap?





plugg inn crm nå. 
bygg opp databasen til to selskaper. ikke mer. nå er databasen
tom. done.



du bruker google sin prediction api. den kommer ikke til å koste deg
noe med mindre dette går strålende, så det er greit. sjekk om det
allerede finnes trenede good_business_news | bad_business_news om
spesifikke selskaper, eller generelt om business.


du får printet ting mellom hver a. ha ha, du har fikset det. content =
articles.content osv. 





make sure the articles are stored to the db. delete everything in db,
and start over.

store the articles to db with their respective companies and
datetimes. urls as well. 

ok. nå spør den brukeren om permissions, som er fremskritt, men når
den skal redirecte til auth_return, så får jeg 404. dvs. at
auth_return er definert i f.eks. javascript.

