du er nå på dynamic. du vil antakelig til resident.


det er i app.yaml at det skjer. her må du definere url'en for det
interne kallet til backenden. dvs. at du må lese litt om forskjellen
mellom app.yaml-url'ene og url'ene nederst i main.py. du må antakelig
også finne et eksempel på en app.yaml

http://stackoverflow.com/questions/9790534/google-app-engine-and-backends-how-to-configure-it-on-development-server


so, i've updated the backend with appcfg, and now it shows in the left
panel. still no msft, though.


remember, if you change code running on a backend, you must run:
appcfg backends <dir> update [backend] to get it to actualy change.


ok, so test is sent to a backend, and it runs without errors. but msft
is still NOT added to db.

maybe try to edit one of the existing entries? might make a difference
if you can't think of anything else.



in cron, test starts scrape.py. (right now, it starts test).



no handler for /_ah/start in main. will lead to 404,
presumably. that's ok. all it does is start the backend.

the actual task can start a background thread. so?

as for the task to be put in the queue: ...





so, you are now calling scrape.py via calling /_ah/start, which adds
scrape.scrape() to the taskqueue. however, putting microsoft in
scrape.py did not result in microsoft being added to db.

so, try it with test.test()



du trenger ikke å handle kallet til /_ah/start. 404 er helt ok. her er
det nemlig fortsatt 60 sek deadline. 

det som starter det som trenger å skje er task queue. 


remember, you can catch deadlineexceed errors to save progress.


ok. det virker. du puttet inn amd i db'en vha. backends! yay! måtte ha
entry i cron.yaml, backends.yaml og handler i main.py. (selve rutinen
er i test.py).


ok, så cronjobben startet uten errors, og utførte arbeid. men amd ble
ikke lagret til db. hm.


du bør ha en shutdown hook i backenden, slik at state blir saved når
ting går ned. det skjer jo før eller siden.


før du roter mer i hvordan scrape, dupes, clean og analyze fungerer,
putt dem inn i en backend og se hvordan det løser et par grunnleggende
problemer!




i think that you have only 1G memory no matter how many memcache lists
you make, so the more lists you have, the shorter they can be. that
means you should have 1 list, and let everyone use that list.

perhaps make a list-maker.py, that runs first, and fetches quite a few
articles from db, and uses cursor. the first one out after that is
dupes, which looks at the incoming from scrape (which scrape stores to
memcache, so now we have 2 lists), and at the listmaker's list, to
check for dupes. 

then, clean takes one from the list and cleans it. then, analyze1 2
and 3 takes one from the list and analyzes it.

listmaker could fetch from db every hour or so, and then the others
add and remove from that list as they see fit. dupes should add those
that are not dupes to the list. clean should do nothing but update the
list and store one to db. analyze should also just update and store
one to db. so, back to the old system then. call the two lists
'old_articles' and 'new_articles' in stead of 'duplicate check' and
'article ids', which were confusing.

store the entire articles in memcache, so that you don't have to fetch
them from db one more time to actually access them.

listmaker can't store all the articles - there is no room. you have to
store a subset of them and use a cursor. but that won't work for
dupes, which needs all the old ones. or can it get by on just the not
so old ones? it probably can. so listmaker only has the last couple of
days' worth of articles. 

this is backend stuff. you should look into backends. frontend stuff
is user interaction, and this is not that - this is stuff that needs
to run all the time in the background.







aller føst: ny memcache-organisering i analyze, som i clean og
janitor. må gjøres i scrape også; den feiler med
deadlineexceeded. probably helps if the first run only fetches all the
urls and stores them to memcache (now you have three lists in memcache).

DU ER HER: split up analyze into several urls that store stuff to
memcache.

del opp i flere funksjoner i samme fil først, og sjekk
locally. deretter, flytt disse ut i flere filer, og sjekk online.





adjust the time memcache stores clean and janitor. no, don't
bother. it stays for "as long as possible".



husk at du nå har en janitor.py, som kjører gjennom alle artikler, en
av gangen, og ordner opp. MEN: du må sørge for at ALLE artikler, både
clean og unclean, blir sjekket, og at janitor progresser over dem, og
ikke henter de samme 5 hver gang. nytt flagg? nei, for da må du en
gang avflagge? eller må du? nei. de som er sjekket forandrer seg jo
ikke.




maybe store all unclean articles to memcache, and do 2 at a time?
remove them from list stored to memcache each time, until list is
empty, at which time you check the db for unclean again. this sounds
good!

use the same principle for analyze; fetch filtered from db, store in
memcache, and work on memcache list until empty, then fetch more
filtered from db.


look into opening up more than one instance, and see how much it
costs.

you must split up the efforts, and make sure that all the articles are
cleaned and analyzed before new scrape. clean takes 2, and analyze
will only take 1, when you get it to work (too much memory now), while
scrape scrapes several. this means you must run clean and analyze
several times for each scrape.


du har fått clean til å cleane, men analyze bruker for mye
minne. dvs. at du må dele opp /analyze i /analyze1, /analyze2, etc. de
gjør en liten del av jobben hver, og lagrer resultatene sine til
memcache.








problemet er nå at clean ikke cleaner. det betyr at den ikke finner
noen som skal cleanes. den bruker article_ids, som først lages i scrape.

lag maintenance.py, og putt old_clean og old_analyze inn der. den går
gjennom hele db'en, og sjekker etter anomalies, og retter på dem. ting
som er clean men ikke analyzed blir analyzed, osv. kjør hver 24 time
eller noe slikt, i development. hver time i production.

old skal ikke være en del av production. old skal ikke bruke memcache,
det er jo en 'nødløsning', som skal sjekke gjennom alt.

du kan, i stedet for å bruke old, kommentere ut scrape, og bare kjore
de andre en stund. da skulle de klare å hale innpå.










display (main) bør ogsa bruke memcache, fordi mange vil bruke servicen
samtidig. hvordan få de ny-skrapede inn i display, da? lage et flagg
som heter new, og slette det når du displayer? ja, det kan funke.


LISTE OVER HVA SOM FAKTISK SKJER NÅ, MED TILLEGG OM HVA SOM BØR SKJE I FRAMTIDEN:

1.a
scrape.articles() henter nye artikler. henter article_ids fra
process_links. henter også gamle article_ids fra memcache, og legger
dem sammen med de nye fra process_links. hvis ingen gamle, genererer
en ny bestående bare av de nye.

1.aa
process_links bruker company.titles. er dette en tvilsom prosedyre?
bør selskapene lagre slikt? det bør vel heller ligge hos artiklene
hvile selskaper de tilhører? compnay.titles er IKKE en
liksom-attributt. der er en ekte, rett fram attributt hos selskapet.

1.aaa
process_links henter og lagrer <201 artikler, og lagrer id'ene deres i
article_ids, som returneres til scrape.articles()

1.b
scrape.articles() genererer også listen duplicate_checks fra memcache,
eller lager en ny, som også inneholder article_ids.

1.bb

scrape.articles() legger aldri enkelt-id'er til duplicate_check. enten hentes
den fullstendig fra mem, eller så legges alle id'ene i article_ids til
den tomme listen. bør ikke duplicate check vokse ogsa? alle article
ids må jo legges til, selv om den er tom, ikke sant?

1.c
scrape skal etterhvert flagge nye artikler med new, slik at display
(main) kan hente bare disse fra db. resten kan hentes fra memcache.
ved restart vil det ikke være noe i memcache, og display må hente
alle fra db. det er bra.

2.a
duplicates.articles() henter article_ids og duplicate_check fra
memcache. går gjennom alle artikkel-id'ene i article_ids, og ser om
den også er i duplicate_checks. hvis så; delete fra db, og fjern fra article_ids.

2.aa
men, vil ikke alltid article ids være forskjellige? når vil de være
like? i så fall må duplicates.articles() hente fra db, og sammenligne
tittel og slikt, som før. mange read ops her. hm. kanskje lagre tittel
i memcache, da.

2.b
hvis duplicate_checks ikke finnes, lagrer duplicate.articles()
article_ids som den nye duplicate_check listen. ellers, lagring ok.

2.c
hvis article_ids heller ikke finnes lagres ingenting.

3.a
clean henter article_ids fra memcache. bruker disse til å hente
artikler fra db, og cleane dem. setter deretter cleanflagget til true.

3.aa
burde det ikke være en flaggsjekk før clean cleaner en artikkel?

3.aaa
clean må lagre artcle_ids igjen. nei? den har jo ikke forandret på
listen. nei, men den må refreshe lagringen av den samme listen, med set.memcache.

4.a
analyze henter article_ids fra memcache. bruker dette til å hente
artikler fra db. dersom cleanflagget er på, analyseres artikkelen, og
id'en fjernes fra article_ids.

4.b
hvis article_ids finnes, lagres de igjen, med set.memcache.






it needs to look good. it can start as a free service, and then you
can add android app, payment, api, better looking text and so on. but
first, and foremost, it needs to look good; it needs to have a central
scrolling section, and a non-scrolling sidebar on each side. it needs
to be white, or close to white, and not piss yellow like it is
now. the font needs to be black for both title and text, with title
being large and fat, with NO underlining. like foursquare, or whatever
it was called, the ipad app that gathered articles from here and
there, and presented them beautifully. your texts need to be beautiful
as well. you don't need them to be complete, or absolutely free from
garbage at first, when it's free, when it needs to be basically
beautiful, so that the first impression, at first glance, of the page,
is pleasant.






tips for later:
til bus-news-an: lagre ordene i en array, tror jeg. deretter, lagre
ord n, n og n+1, n og n+2, n og n+3 og n og n+4. dette er kanskje den
mest effektive måten å lagre sparse matrices på. se også på nett om
det er noen etablerte måter.

look into using split() to splitting the sentences into words. might
be an efficienty boost here, unless you are already using it.

do you want to store the word pairs, with their frequencies, in the
db? would that be good when the db gets big? would save a lot of
processing, but would also cost a lot of read operations. you could
make the entire thing a dictionary, and store that as one entry,
though. is a dict as efficient as an array? well, store it as the most
efficient thing, and only as one thing. try that.

------



i have commented out /scrape and /test from app.yaml. they are
probably not needed.


does scrape have to run more than once? if so, does the others need to
run once every time scrape runs?

duplitaces.articles - du har begynt på nytt øverst. les gjennom og se
om dette gir mening, og sjekk om det funker. hvis det funker skal dupe
check listen lages, slik at analyze har noe å arbeide med.


is this still a problem? i think so.
no, wait. it's no problem that it removes the ones that are
dupes. however, it's a problem that it only does anything if there
already exists a dupe check list, which means it never actually
checks, because the dupe check list is never made (by dupe).


ok, the problem is that dupes removes the dupe checked articles from
both duplicate check and from article ids, so that both those lists
are empty when you get to clean.

btw, what the hell are you using duplicate check for, anyway?

so, you are not really changing duplicate check in scrape, only
article ids, onto which new article ids are added. seems legit.

clean does not alter article ids. it just changes the clean flag.

analyze removes articles from article ids.




you need to look through the chain: scrape, then dupes, then
clean. analyze seems to work after clean, but there is a problem
somewhere before clean. memcache, probably. scrape or dupes is not
storing properly.






check.html skal displaye de forskjellige ctrs (ctr[0], etc.) fra
utils.check.



lag en kort, enkel rutine som tar absolutt alle artiklene, teller dem,
og printer resultatet. hvis det funker, og du får samme resultat som
articles.count(), så utvider du den, slik at den etterhvert kan finne
alle som har feil i seg, og rette det opp.

siden dette problemet strengt tatt er rydding i databasen så kan du
drite i det en stund, og bare kjøre videre. jobbe litt med sparse
pairs, f.eks. (søk nedover etter sparse pairs og nedover.)


check is supposed to go through all articles, and set analyzed to
false if there is no sentiment. this does not do anything. the time it
takes for it to finish is too short, like with analyze old and clean
old. they do not hit anything. why???????????????

lag en rutine, basert på analyze all, som går gjennom alle, og sjekker
både ditt og datt - er det text, har den et selskap, har den
sentiment? hvis ikke, analyser. hvis ikke selskap - dropp it.

are there zombie ids in articles too?

perhaps wait until all the memcache is empty, and try analyze old
again. see if i avoid passing the soft memory limit then.

there are as many analyzed:true as there are clean:true in the view,
so there are obviously articles that have been tagged as analyzed even
though they are not. (i did that, in main, a while ago. no
mystery. but it needs to be fixed.)

still absolutely no dice on analyzing old articles. dammit!

du kjører analyze old online, og øker chunk size.

holder på å kjøre analyze all. det virker som om det er en memory leak
der. (det ser ut som alle er clean, siden alle har tekst - det er de
som har analyzed = true som blir displayet - men vil ikke det si at du
kan sjekke analyzed == true før du analyserer?)


du prøver nå clean old og analyze old uten filter, og på cron hvert minutt.

clean = true, analyzed = true. jeg putter altså ikke inn tekst når jeg
cleaner, eller noe slikt.

maybe the cursor isn't moving along?

clean old and analyze old worked locally. why not online?






det er fordi du ikke nullstiller cursoren igjen. du må delete
cursor. nei, de blir fortsatt ikke cleaned, og færre analyzed, selv om
jeg sletter cursoren.
apple: 22 av 231
google: 15 av 199
face: 107 av 170

har kjørt clean old og kjører analyze old online nå, men det ser ikke ut til at
noen blir analyzed. de er cleaned, men blir ikke analyzed.
apple: 17 av 99
google: 15 av 74
face: 37 av 49



det som skjer nå er at dersom jeg kjører scrape to ganger på rad, så
faller artikler ut, og må kjøres vha. scrape_old_articles. slik kan
man muligens ikke ha det, i alle fall ikke during utvikling. eller
hva? kanskje? så lenge dupes, clean and analyze alltid kjøres etter
scrape, og alltid i den rekkefølgen, så må det vel gå bra?

nå virker det som om artikler som har blitt skrapet, men ikke cleaned
eller analyzed, har article.clean joda, det var bare article.count()
som ble brukt feil i company click handler.

put in an is clean check (actually is analyzed) in company display, so
that only finished articles are displayed. company click handler i main.




corrected article ids must be added to the article to analyze id list
in memcache. and, i guess, to the duplicate check list? no, i don't
think so. these are checked, and old, remember. but they could be
quite new, and on both lists, or on the dup list but not the
other. no, they can't be on any list, because then they would not be
displayed.


add a line in dupes that eats a little from the old end of the
dupe check article id list if the list is really long. say, if the list is longer
than 300, eat 250. (the analyze check list is cropped by analyze every
time, so only the dupe check list.)






irrelevant now?:
correction cannot update immediately. ok, so only update every hour,
but run the four (scrape, dupe, clean, analyze) every ten minutes. for
now, scale this up to where you don't hit the limits.














har lagt in memcache for articles i scrape.

put in memcaching of all articles. each chunk takes care of its
own, but you should try to store them between chunks.

each of these can pick up all articles from memcache, because the
previous one stored the same all articles TO memcache.

you run an update db from time to time, which  fetches
everything from db - in chunks, with cursor - and compares with all
articles rfom memcache, updating and adding entries to db as
needed. NO removing.

dupe takes care of its own removing - updates the db at the same time
as the memcache (not costly).


start with articles. all four programs need to do it.

scrape: fetching company.all, ---	adding articles do this
clean: article.all, ---       	   	updating articles do this
analyze: article.all, --- 	 	updating articles do this

dupes: company.all, article.all, --- 	removing from both; only do articles for now.



company click in main: none ---		adding companies. not now.



det at sentiment og text var none for alle artiklene var fordi clean
og analyze ikke hadde tatt alle ennå. nå tar de fler av gangen.




du lagrer både article.company og company.titles. dette er dobbelt
lagring, og med motsatt metode. gjør det på 1 måte på 1 sted. m.a.o.:
hent title fra article.company. lagres i process_links i scrape.

still sentiment none, text none, for articles that have been scraped,
duped, cleaned and analyzed. why?






see notes on phone about sparse pairs..

try to do windows of five and sparse pairs. it would still be baysian,
but with better tokens.

your correction is working well enough - your correction is taken as
gospel right now, and that's fine.

display the result of all four analyses, and keep track of their
accuracy. build something that does that.





fetch new articles by timestamp, old ones from memcache - this is in analyze.

se om du klarer å legge til trettifem minutter memcache i analyze,
slik at den varer til neste cron job. du jobber i analyze.pf, og har
kopiert inn et avsnitt med gammel memcache.

legg til de nyankomne før du lagrer til memcache, og lagre alt til db
med jenve mellomrom. hent også alt fra db med jevne mellomrom, for å
sikre at du er a jour.

deretter dupes. 





no graphics - the web app should only deliver the data, not the
presentation - the presentation is the domain of the android app.

the api subscription should be a lot more expensive that the usual
one.

don't display as default. index.html should not display anything (for
now, it's ok, though, during development - but NOT during production -
then, only the android app should display).


forskjell i ratio fra siste tidsperiode. - senere.




autofokus i tekstfeltet!


du displayer nå artikkeltitler for hvert selskap når de blir klikket
på (dersom de har artikler).

du jobber i company.html. articles.html har skiftet navn.



i db'en blir de nye artiklene lagt til i slutten av listen, så de vil
alltid være ETTER cursor. altså ikke noe problem å bruke cursor slik
du bruker den nå, uten et flagg som sier 'start over'. dette gjelder
ting som analyze, som gjøres bare en gang pr. artikkel. hm. gjør det
det? du analyserer vel alle selskapene om igjen hver eneste gang?
eller? nei, du gjør ikke det.

du vil altså to ting: du vil ha cursor, som ikke er noe problem, siden
nye entries alltid legges etter cursor.

legg inn cursors i analyze, clean, dupes også. i main har det nettopp
skjedd noe (i correction, scrape, clean eller dupes), så da må du vel
hente på nytt fra db? nei, ikke så lenge de nye får høyere id enn de
gamle, da vil de fortsatt være bak cursoren.

for det andre vil du lagre de gamle entryene i memcache i ti minutter
til neste runde, og legge til de nye entryene før du lagrer til
memcache igjen. husk å legge dem til i db også, ikke bare i
memcache. og husk å lese fra db av og til, for å være sikker på at
memchache er korrekt.




analyze fører til 4 - 7% per gang utført! memcache på alle man har fra
før - du kan jo legge til de nye også, og holde dem i memcache i ti
minutter, til neste gang.
dupes fører til 1.5 - 2% reads per ti ganger utført. memcache her
også, på de gamle - her kan du også legge til de nye.
n = seconds
stackoverflowspørsmålene dine - der er er dine egne
memcachelinjer. viser syntaks for bruk av seconds





kjør en article.All() og display length av denne i main, for å se hvor
mange artikler du har i db'en (online). length var 53 online, altså
ikke særlig mange.



reduce read operations by 66%. if not, reduce frequency in
cron. splitting company display and article display in two will help.


when i'm finding current user in the db, i'm not getting his nick and
email.

you now have articles for each company, and companies for each
user.

after that, you are ready to start organizing the display of companies
and articles. do the company page first, i think.


to create the article display page, må du bare endre
den ene linjen som nå henter alle artikler, men som bare skal hente
artikler for ett selskap. the line is ready, but commented out for
now.




whenever user logs in, check if he's in the db. if not, put him
in. every time he buys another subscription, put that in.






you have put is_prose back in.

no, you can have both for now:
you must make the program decide which one of the two sentiments it is
going to use. simply take the one which is the farthest from 0.5.




you need to analyze an article as soon as it arrives - it is pointless
to display articles without an analysis. perhaps delay the display
until analyze flag is set? try without for a while and see what it
looks like.


this is how you should retreive articles: articles =
company.articles.get() articles are attributes of companies. that
means you must make this work in models.py.



texts are still not very well cleaned, but save that for later. you
can clean the html, which you have stored, with the new cleaning
regime. that means you should stop deleting articles, but just set a
flag to not display, or just delete their .clean attribute.

you are not getting the text under sub headlines - only the first
chunk of text under the first heading is captured. for now, this is ok.
you are getting the entire thing in article.html, you are just failing
to extract all of it in clean.py













you are now working on the cleaning - to get the text, the whole text
and nothing but the text. look at stripped_strings. dette er strengt
tatt optimering, men greit å ha noenlunde greit utseende tekster å
jobbe med for min egen del.

for every article in db, if not clean flag, clean, then break. do it
again ten minutes later. this way, you get one at a time, and no
deadline exceeded.


then, make sure you clean a handful of articles at a time, and not the
same ones. i think this is already in place. it cleans many, but that
seems to work out ok. it won't clean that many when we start scraping
regularly anyway. no, now, it's only cleaning 1 at a time. do more at
a time later.



du kan gjøre samme trikset i scrape som du har tenkt å gjøre i clean;
nemlig å lage en loop som tar to, tre stykker (i scrape - bare 1,
vha. å sette et flagg, antar jeg, eller skrape listen av titler, og så
gå gjennom den, sette flagg i memcache eller noe slikt?, eller bare ta
duplikatsjekk på tittellisten?), og deretter breaker, og
så gjør det hele en gang til noen minutter etter. da må du ha
duplikatsjekken i orden, altså, slik at du ikke stopper på de samme to
hele tiden. etterhvert vil du jo skrape den sist ankomne av det
selskapets artikler, så kanskje bare sjekke at du bare tar den sist
ankomne (set() føkker rekkelfølgen), og duplikatsjekke om den tittelen
finnes i db'en? men det kaster du bort en db-query på... memcache?.














hva med en try: encode 5-10 vanligste encodings. except: fuck it, too
unusual. litt som case.

det er også mulig at problemet er så enkelt som andre linje i
scriptet, den som sier at det skal være utf-8. kan være at linje 3 er
den korrekte (med en hash færre).


then we're back with the always exiting ascii encoding horror. the
many fails in fetch are due to ascii. fuck me.


right now, scraping the entire html works on one out of ten. the
others are getting 'except in scrape'. locally and online. it seems
result.content does not have a type - at least not one that lets
itself be printed. StringIO lib?


you really do want to store html (or soup objects) so that you can
soup them and clean them at the same time.

have another look at whether you can store result.content (a string)
in test.py. you SHOULD be able to, since it DOES contain the html
string for the page, and you are able to soup it.

you don't really need two different storage places for the clean and
unclean texts, though. the ones with clean flags will be left alone.

start working on clean. let scrape be considered good enough for
now. improving it is optimization, after all.

store the scrape as a blob? or a string? the string bleeds into the
html of my own page...

include clean flag in db.




using stripped_strings. most articles are now full of text, but the
odd one out is 'except in scrape'.






du har en bs4 side åpen som beskriver hvordan finne all tekst i en
suppe. men det var for bs3. get_text() skal gjøre samme jobben.

even when not extracting any tags, some articles are just a single
line. look at which ones they are, and try to figure out why their
divs don't seem to contain anything.
valuewalk: jumble, jumble, jumble, jumble (de gjemmer teksten i en
script-tag.) fikses i fetch, som bare gjør get_text på soup'en.
abc30: title only - de har teksten i en <span class="storyDateline">
dailymail: title only - font tags inside p tags NOT inside div. crazy shit.

har du tid til å extracte script, style o.l. rett før linjen text =
soup.get_text() eller deromkring? eller vil du få deadlineexceeded?










some article texts disappear with divs - only the boilerplate links at
the bottom of the page are included (valuewalk) deal with this
later. this is optimization.





har spoofet headers. får artikler. bra. men: rekkefølgen på div'ene
som kommer ut er rar, og det er mange duplikater. f.eks: fetch printer
div 3, deretter første halvdel av 14, deretter siste halvdel av 17,
deretter 21, deretter printout fra funksjonen som
fetch returnerer til (scrape), deretter resten av 14, 15, 16 og første
halvdel av 17.


unngå 403: gjør som eskil gjorde, og fortell at du er firefox sånn og slik.


drit i å cleane teksten mer nå. få det du har til å virke online, og
clean mer etterpå.

du jobber i test.py.

du holder på å prøve å cleane teksten i det du henter den, vha. bs -
slik at den er ganske godt cleanet før du sender den til
clean. hvorfor vil du det? fordi jo mer jeg kan gjøre bare vha. bs, jo
mer robust og elegant er det. clean er noe jeg bruker på noe som er
veldig rotete - hvis jeg kan gi clean noe som er mindre rotete, så er
det bra.

akkurat nå får du veldig mange 403, og når du får tekst får du fem
eksemplarer av den.



the ids are 506, 516, etc. way too high for the ctr. how to fix? get
minimum, and then increase? get the range too, by getting the max.





third, clean articles - this will take care of documents.write('

third and a half: prose check

fourth, plagiarism check

fifth, analysis.







when this is ready, you can leave the system running. it will then
gather articles, and you must read them and categorize them
manually. they will be displayed at the / url, with their buttons. all
the finished, cleaned and duplicate checked articles will be there.


























first, you are going to see if you can get it to complete without a
'failed' message. you are going to try two different things in order
to accomplish this:

1 - you are going to download "related words" to dictionary words from
dictionary.com; 

2 - you are going to see if you need to have the handler in main, and
if the redirect in necessary. try to put the handler in scrape.py, and
drop the ridirect (drop the redir. first).

































fetch an image from a different webpage. display it. add one element
in the definition of Article() to store the image.


just hit reload online (not /scrape) to see if 2 new articles (one per
company) have popped up. there were new articles. hm.



btw: try to enclose the content of the article in triple quotes, to
avoid messing up things in display and elsewhere.



now, there are articles, even though the cron panel says the cron job
failed. there are also no errors in the logs.







change scrape back to just taking one article each time. run every
five minutes or so.

you have changed it to try to get no errors when things don't turn out
like they should - inserted a continue statement, for
instance. continue this if it doesn't work on the first attempt.



so scrape completes when run manually, but the cron job fails to complete.



run only sites.py, and see what it returns. then run only fetch, and
see what it returns. you can run similar things that fetch just a
simple image from a different webpage too. this will expose the bugs
in your scraping code.


una balloona. fetch feiler der. du er her.









you made the cron (scrape) run online. now, make all the
normalization, duplication checks and analysis run too. cron job
without web request, and hope it doesn't time out.




you are now in scrape_old, to try to make that work before you jump to
scrape. it doesn't - util.is_prose().


you are now in scrape. remove everything from there, and take only the
essentials from scrape_old; that means scrape one article and print it
on the web page. no storing or processing or analysis or anything








scrape_old.py contains everything from scrape. you can now start
hacking away at scrape, removing all the superfluous stuff that
happens - only scrape, nothing else, and only the first from each
company.

the get self method in scrape is 1 request - everything that happens
in it is 1 request. that means it should do as little as possible. let
it scrape a few articles (maybe use a counter stored in db to go
through the various companies - jus one at a time (the counter decides
which one), and return it to main, which stores it in memcache and
sends it to another handler (NormalizeHandler, then DuplicateHandler,
then ProseHandler, or similar). even simpler - main can store it do
db, and then delete it again.

make a fetch_old.py too.


make a new file that only scrapes the first article from each company,
and only that - no normalization or analysis, just scrape, return and
hold in memory.




you have removed txt temporarily from fetch, to see if that will fix
the deadline problem. you could also remove other time consuming
elements from the chain.

things like the duplicate checking and so on. perhaps scrape
everything first, and then do all the normalization and duplicate
checking later? that could solve the problem. especially if you only
scrape the first article for each company!

















perhaps the fail to recognize each url fetch as one request is due to
something in fetch.article()? perhaps you shouldn't go via
fetch.text() before returning from fetch.article()?



what if you just let the scrape be for now, and work on displaying and
selling what you already have to different accounts? that would let
you finish the display bit (no fancy design), the selling bit and the
login bit.






you have asked a question to doeiqts on stack. he had the same
problem, and it seemed to get fixed.





you now have a cron job running online. try commenting in the lines
that fetch companies, and see if that works, if this doesn't.


this is what you want:
https://developers.google.com/appengine/docs/python/taskqueue/overview-push
your last tab is open to this right now.




so, maybe the task queue is the thing to use, backends might be
overkill. no, because the problem is with the size of the thing you
want to do, not starting it - you could start it from cron.
so:
a - define your backend in backends.yaml
b - update backends with the appcfg tool
c - start the dynamic backend manually with google's admin console.
only after this will the backend actually run, it seems.

on the other hand: "dynamic backends start automatically when they
receive a http request", so you could schedule that request in cron,
and let that start the dynamic backend, which can then run for as long
as it wants without hitting the limits you are hitting now
(deadlineexceederror...)

use taskqueue.add(url='<path to handler>', target='<backend name>') in
the frontend app, but then again:

a taskqueue can run for 10 minutes, so you might not need backends
after all.







you can target the backend id from cron. use the name of the backend
as a target in cron.yaml


f you're confused about how to actually run code on the backend, your best bet is to configure a task queue to run on that backend with the target parameter, and then get your frontend code to trigger a task on that queue.


meget mulig at du veldig enkelt kan starte scripts ("scrape.ssh" or
whatever syntax) i backends.yaml. det betyr at du bør kjøre som
backends på tri, IKKE online, gratis, for å trene systemet, og så ta
deg råd til å kjøre backends online sporadisk og se om du får kunder,
deretter kjøre oftere for å sikre updatedness i scrapingen.

begynn med en ganske tom /scrape, og print små ting etterhvert. først
listen over links, så den første artikkelen, så noen flere, så alle. sjekk
at hvert steg faktisk virker online. cron-jobben starter f.eks hvert
30. sekund eller noe.

en cron-job starter /scrape! den startes IKKE fra main. scrape viser
ingenting heller (men du bør selvsagt printe ting manuelt under
utvikling (drit i å lage en egen scrape.html)).

main skal bare displaye user requests, mens arbeidet foregår i /scrape
og etterhvert /bayesian, som begge startes vha. cron-jobs.

da må du legge companies og article objects inn i scrape, og fjerne
dem fra main.

uansett hvordan du bestemmer deg for å gjøre /scrape; backends,
asynchronous, etc., så blir den startet av en cron-job. (backends
defineres i backends.yaml).

backends er dyrt (billed by the hour, even if idle), så prøv å gjøre
det vha. cron og /scrape requests først. gjør det vha. backends hvis
ikke mulig å dele opp request-loopen i enkeltrequests fra /scrape.

på den annen side: hvis du bruker dynamic backend, og kjører den en
eller to ganger om dagen (til du begynner å tjene penger), så blir det
ikke særlig dyrt.






du leser denne siden, om requests:
https://developers.google.com/appengine/docs/python/urlfetch/
og denne, om deadlineexceeded error:
https://developers.google.com/appengine/docs/python/

or, how about using a cron job to "call a specific url of the
application" at regular intervals, and make that url (/scrape) run the
whole for-loop. that was your original idea.

hvis det ikke funker, fuck it, make it a backends.





make the html = urllib2.urlopen().read() line in fetch into a
request() instead. that will hopefully make each of those read
requests into 1 request that actually counts as 1 request.

or do i have to have a redirect or something to make it count?


getting 403 on articles from valuewalk. also when running python in
terminal. hm.


url'ene fra links gir file not found. manuell url går bra, da stopper
den i return-statementene du har lagt inn. se hva som skjer dersom du
fjerner returns. da bør du få masse duplikerte artikler, eller
noe. kanskje de blir filtrert ut og du bare får en (ideell case,
dersom filtrene mine funker). gjør dette og se hva som skjer!

try this: just do the first urlopen in the for-loop, and then return
(two return statements; one in each for-loop). see if that works
online, not just on your computer. then work from there. perhaps next
step: try fetching only no. 5 in the loop, or something.

how about redirect, and then redirect back? no, that will reset you
back to the beginning of the for-loop, won't it? 


why is urllib2.urlopen() not a http request? it is, but why does it
not count as 1, but rather as part of a long, big one - where the
entire for-loop is 1 request?

eller en fullstendig request() med alle nødv. argumenter i linje 88.

or, perhaps even better, in line 73 in fetch:
    html = urllib2.urlopen(url).read()
should be made into a request() line, in order to make it count as one
    request, and therefore split the requests into many, and thereby
    not use much time per request, and thereby avoid timeouts (dealineexceedederror).

i stedet for linje 88 i scrape kjører du en redirect til /scrape, som
kjører den ene linjen "article_text = fetch.article(link)", or
redirecter tilbake til /. dvs at du må kjøre denne redirecten for hver
selskap, og for hver artikkel, på riktig sted, slik at du får gjort
dem i korrekt rekkefølge, og får gjort alle.


scrapehandler er for stor. den tar jo alle requestene for alle
artiklene. du må ha en som heter noe sånt som fetchhandler, som tar en
artikkel av gangen, og som kalles fra fetch.article().

ser det f.eks. slik ut?:
url = "/fetch"
result = urllib2.open(url)

DU ER HER:
in scrape: for each company: for each link, hit the url that fetches that 1
article. that means each request takes only a little, time, and you
won't run into deadlineecxeedederrors.

det er scrapeHandler som tar seg av dette. finn ut hvordan du
initierer scrapeHandler, men uten knapp, unlike CorrectionHandler, som
initieres av knapp på siden.

DU ER PÅ LINJE 88 i scrape.



scrape comes up against the DeadlineExceededError. see what that is,
and if you can modify the deadline. if not, you have to scrape in
smaller chunks. du må kanskje bruke backends til dette, siden de åpner
for requests som tar mer enn 60 sekunder. hvis du lar selve appen ta
seg av slike requests vil det skalere dårlig. på den annen side skal
det jo ikke skalere - det er jo bare en 'bruker' som gjør disse
requestene - alle andre brukere vil bare ha display, og gi correction
feedback.



se på notater på telefon og i sort bok. du holdt på med å migrere hele
greia til en enklere versjon, der du i utgangspunktet bare skal hente
artikler, mener jeg å huske. så bygge på småting en etter en.



del opp appen i veldig små biter; innlogging, betaling, mer effektiv
skraping, design, kanskje integrering... bruk nettsiden der du lagde
konto og betalte fem dollar; gi folk der små oppgaver først, kanskje
til og med to, tre forskjellige folk til samme oppgave. lag ny
githubkonto, og lag selv ferdig en side som viser det brukerne ser (og
legg den ut på github.) få noen til å designe utseendet, og så
betaling for å abonnere på tilgangen til dataene. så noen til å fikse
innlogging, slik at folk ser det de skal, og bare det. riktig
rekkefølge på innlogging og betaling? ja, tror det. nei, kanskje
ikke. så gjerne effektiv skraping (lag skraping selv først, oppdelt i
mange requests, som burde funke, i motsetning til det som skjer nå,
der du får timeout hele tiden fordi alt er en eneste stor
request). analysen lager du også selv, men du kan be folk lage
markovian, og kanskje en variant av hyperspace?

ikke vær redd for å ta deg godt betalt for appen; tenk på hvor mye
penger du bruker bare på mat på vei til jobb. gjerne 40 dollar for
appen, og 5 dollar i abonnement pr. selskap.


note that the logging printouts you have put in are printed in the
logs. yay!


you are in scrape. it will become a file that handles the
scraping/fetching of articles, and only that. and then redirects to
/.*, which it already does. see ScrapeHandler in main.


make a new file for the fetching, and do only that - give it its own
url, and put it in cron.yaml.

divide the tasks: first you fetch and store. then, you clean and
check? hm. must check for duplicates, and i can't store enormous
amounts of text? i can for a while, if the user never sees the big
blob of unclean text. i can then normalize and check it, and then
either remove it, or clean it and overwrite it in the db. then, i can
calculate probs. nicely divided.




you are debugging importing bs4. you have a stackoverflow page open.



it's the ones that are not properly printed/formatted - the ones
without the END tag that don't work. Not to worry, take care of that
later.



Error Because of missing httplib2, which is only used for the
prediction stuff. try to remove it and see what happens. do it on your
own machine first, and see.


you have emptied the uploaded cron, to avoid things happening that
might cost money.






you messed up by changing the name of the app from nameoftheapp.... to
transfarmr. now, it can't access its own data. read up on it, and
change it permanently, even if you have to change stuff with your ip
provider to get it right.




cron.yaml is perhaps gold.


you need to find a way to schedule things such that all the fetching
and calculating happens behind the scenes, while the web page only
displays the contents of the db.

next is making sure the whole cycle isn't initiated each time a user
accesses the page, (or hits the correction button, which is the same
thing). that means 







i think perhaps i am storing the new sentiment to db now. that means i
should look at the flowchart, or make one, and see what needs to
happen when, and what i need to pull out of the db in order to present
the new articles properly.


next up is to remove dummy sentiment tagging, and actually use the
user's tagging as correction. 



you are using comp_arts to see if a company has any articles attached
to it. it doesn't. dammit.



you are getting there, but you have a bug. even though the article,
with content and sentiment is displayed, the article object is a
'None' object.



you have a few answers. couldn't get any of them to work tonight, but
i learned that this syntax is complex, and will profit from being
researched by you tomorrow. in addition, article.put() will return the
id:

art_id = art.put()
art_id yields f.x. 136522L
Article.get_by_id(136522) #without the L








you want to return article.key from index.html, not the whole
object. you sent the article object there, but you don't need the
whole thing back. you only need the key in order to be able to fetch
the object from db so you can manipulate it.


<!-- <button type="button" name="**{{ article.key.id
}}**">Delete!</button> --> or just article.key

this looks promising (talking about how to have a button under each
element from a list in the template)	:
http://stackoverflow.com/questions/4059965/google-app-engine-get-entity-key-to-use-in-a-template

i am going to just let the web app fetch articles and such, and store
them and their attributes. then, let the kivy app access the web app
and display things, and update their sentiments.


uncomment the fetching of new articles for this - you only need old
articles and words. the user is only ever going to fetch from db,
never initiate fetching of new articles, after all.
you are probably going to use the template language for
appengine, like you have in index.html. read more about that template
language, and find out how to insert links/buttons. 'input' is a key
word here, since buttons are called input in html.

put in a link to delete the article as well.



when this is finished, see if there are two companies, and only one of
each.

then, uncomment the line in index, and see if you can display the
articles for each company. you can't, but fuck it. look at it later.






much later, you can make a duplication check that fetches the ten least
common words from an article, and compares them to the others in the
db. if, say, six of them are the same, note that they are possible
duplicates. the twenty least common words from each article can be its
dna, an attribute in the db that you can match against other articles.








maybe use zencoding-mode for emacs for the html? see carlgroner.me
(and then the readme at the project page).
it seems the buttons must be written in html. sigh. well, ok, there
aren't too many buttons, right, so i'll do that.
















ok, så de blir lagret. de blir antakelig ikke duplisert, siden du
akkurat nå har to fb-entries.

you are now running one to see if the article containing "been
under fire in recent" is displayed. also, the one containing "bringing
free data access to messaging". if they are, it means articles are
stored to db, despite them not being displayed.

so, the selection changes, which means new articles are fetched. but
are they stored to db?

see how the selection of articles changes when you run it again
tomorrow. the current ones, from 1300 on wednesday march 6., are at
the top of tmp.py. you expect them to change if things work; new ones
will be added.











avoid emptying the db. if necessary, have a dummy-insertion
function. empty db gives you all kinds of list index out of
range, divide by zero, etc. errors.

one of each category should do the trick, complete with link,
sentiment, text, etc. def seed_db().

kjør et par ganger til, og se om det blir fylt opp noen positive
artikler. koden skal funke nå, og jeg har tømt db'en, så det er klart
for ny oppfylling. startet en før jeg la meg fredag kveld.


siden jeg bommet på ctr en gang, og ingen ble tagget som positive ble
positive_article_texts ikke fylt opp, og derfor ble pos_size 0, og
derfor ble det division by zero.

når du får tagget noen artikler som positive vil vi ikke lenger ha
division by zero.

du har slettet artikler fra db, se om det dukker opp noen nye article
objects to db nå.



sjekk hvorfor article_objects_to_db ikke blir lagret skikkelig. de er
tomme i displayet.



deretter: display positive/neutral/negative korreksjonslinker etter hver
artikkel.










make some utit tests that check whether the stuff you think is being
stored is actually being stored. because article_objects_to_db
certainly ain't.







db'en er god nok. det gjør ingenting at det er en del rusk i
db'en. ikke før du begynner å optimere.

derfor: begynn med mekanisme for å korrigere manuelt. dvs. at du må



aller først: lagre article prob til db.DONE.

fjern ord som begynner med " eller '




ordene kommer ut av db'en. bra.

du må fjerne tall outright, og helst boilerplateord som f.eks
cnnadcreatead183812, fra db.





det ser ut til at den går inn i en uendelig loop nå. du prøver å lagre
words, words freqs, word probs til db.

etter at du har fått til det lagrer du artikkel probs, som du allerede
har regnet ut, til db.

gjør all inn og ut av db i main. det koster ikke noe ekstra
prosessortid, og er oversiktlig.











after you do the above, you must figure out why a couple of the
articles are not displayed completely - START is there, but END is
not. maybe not important, since it's just display? one of them is
totally empty, though.


you need to do a couple of things:

fetch non-normalized text for display.

fetch normalized text for classifier and duplicate check.

that means two entries for each article. ok, do it. just add
article.normalized.

you also need to do things in the right order.
you have a db with
-non-norm articles
-normalized text
-neg/pos tag
-prob
another db holds:   DU ER HER: putt ting inn i denne db'en.
-tokens with:
-prob
-pos frequency
-neg frequency

you fetch non-normalized text:
-normalize it
-duplicate check it (fetching norm from db)
-classify it
-put the results in db.

when a user requests it, you fetch a company's non-norm arts from db:
-display them, with:
-frequencies (2green, 10 grey, 0 red)
-first sentence, eventually title







now, you should build the normalization of the text, for the
classifier and the duplicate checker.














good cleaning now, but the counting of articles is broken. counting 9,
showing 0, then counting 10 and showing 7. is it a real problem, or
just a fluke?

expand the cleaning to do well on other sites.

work on that only a little, then store the tokens to the db, with pos
freq, neg freq and prob. if they are unknown, so be it, just store the words.





you got a pretty good text by commenting out the stuff looking for
weird words in remove_outright. put that stuff at the end, then, or
even back in is_boilerplate.






you are working in clean.py. you have managed to remove sentences that
are offensive. next thing is to remove everything that is very
obviously offensive BEFORE you remove those sentences.







you need to work on cleaning for a bit. you need to have two different
cleaning schemes; one for the duplicate checker and the classifier,
and one for the user.


objektet er et ord. attributter er freq_pos, freq_neg, prob.

bytt rekkefølge på de to under. lagre ordene til db først, slik at du
har alt materialet lagret til db, og kan jobbe derfra. tagging fra
brukerens side er tross alt noe som kan skje lenge etter at artikkelen
er hentet inn og klassifisert.

next up is tagging the displayed articles with positive, neutral or
negative, by using links displayed next to each article.

så lagrer du ordene til db, med prob.

da får du et korpus av taggede artikler, og et korpus av ord med probs
(ikke tilhørighet).

så henter du inn ny artikkel, og henter ordene fra den og legger dem i
en foreløpig frekvensliste. så henter du frekvensene fra db, og
oppdaterer den foreløpige frekvenslisten, som du bruker til å regne ut
artikkelens prob (sammen med korpus-størrelsene, som du også henter
fra db).

så putter du artikkelen i db, med tilhørighet og prob.




den eneste forandringen en bruker kan gjøre i db'en, er å korrigere
den automatiske taggingen. (i tillegg til å legge selskaper til sin
watch list).



next up is total duplication filter, so that you don't have to empty
your db all the time. you want to put articles in your db, and
display them with three buttons/links: positive, neutral,
negative. when you tag them, you want them to stay in your db.

check whether the db is transferred when you upload your current
version to the actual website, or if everything you do on localhost is
lost. in that case, you have to do all your tagging online.





you can use freq_pos and neg to see what words occur many times
without being relevant, and then you can cut them away in
clean.py. 

before you break the sentences into tokens, you should remove
everything that is not a sentence - that is, anything that does not
end in .,?!: - perhaps avoid ;, since it is used in
javascript. however, it is also often used in prose.






1. get corpus size from db. use the whole bunch of articles, never
mind whether they are pos or neg. display it on the web page.

count = modelname.all(keys_only=True).count()

count = modelname.all(keys_only=True).count(some_upper_bound)



2. use count tokens to get frequencies. divide in half to get npos and
nneg, since the corpus is not divided in two (three) yet. display that
too, on the web page.

3. finish verdict(), which is described in the first paragraph
below. display that too.

when you pull a word out of the db, you check if it has a sentiment
tag. if it doesn't, you run it through verdict and give it a
tag. then you should send it to human correction.



to calculate the prob. of a text, you need to know the probs of all
the words in that text. is that simply the average of all the word
probabilities, or what? what does bayes say?

bayes says: the combined prob of a, b, and c is (a*b*c) / (a*b*c + (1 - a)(1 -
b)(1 - c))

in order to know the probs of all the words in the text, you need to
know the four things (for each word):
size of both corpuses.
frequency of each word in both corpuses.

so you want one function that takes a word, and the four things, and
returns a prob for that word. this is find_probability()

then, another function which takes all the probabilities for the
currently relevant words, and calculates the total probability for the
whole text. 

in order to get word frequencies from the corpuses, use
count_tokens().

in order to get corpus sizes, use db syntax.

after you have done this, you can start putting neg and pos tags into
the db entry for each article when you store them. this bit is not
needed for just cobbling together the calculating of probabilities.








you can use the dict in naive bayes count tokens, and just add the
probability to the value list.

then, maybe store the whole dict somewhere.












artiklene blir nå hentet skikkelig.

du kan nå hente artikler fra databasen, og klassifisere
dem. etterhvert som artiklene blir klassifisert, vil ordene i dem 
få en 0.01-0.99 verditag i valuelisten i artikkeldict'en til det
selskapet. samtidig vil artikkelen få en pos/neg tag.

når klassifiseringen funker, men dårlig, legger du til brukerkorrigering.



you need to have the storing of the article text (and its tag
(pos/neg) to the db in order to be able to work sensibly on the
bayesian filtering.

the reason why the links are empty is in sites.gf(). it returns empty
links. nei, det er ikke det. det er at linkene som hentes fra sites er
i unicode, og inneholder html-tags for & osv, og det klarer ikke fetch
å bruke til noe vettugt. må vel rense linkene før de brukes, da? eller
sørge for at teksten, og ikke html'en, hentes.





work on bayes probabilities for a while. counting tokens is next.




checkout in tidligere versjon; se at den virker. diff den og
current. se hva forskjellen er, som gjør at den ikke vil displaye
tekst.

det funket ikke: se på en som viser links, da, og se om du kan
forandre bittelitt for å få tekster.

når du har fått displayen tilbake, så kan du lage en naive bayes uten
å involvere hele web-greia. bare for den noen tekster, og se at den
gir konklusjoner. da kan du plugge den inn i web-greia, og trene den
til større presisjon.




txt ut fra fetch.article(link) er ikke "None", men heller ikke tekst,
selv om den skulle være unicode.

finn ut hva den er. kjør f.eks. programmene uten alt web-pisset. bare
kjør sites, fetch, classify, og når det funker, så putter du det inn i
web-servicen.

on the other hand, you won't have the db without the web engine...









content er ikke tom, men fetch.article(link) henter ingenting. dersom
du setter link til a være content, så funker det, så content blir
stored dersom det er noe i den variabelen.


ok, så article.content er faktisk tom. det funker med andre ting (url,
"egen tekst"), så da er det rett og slett tomt i
article.content. hvorfor blir ikke teksten lagret der, da? den blir jo
lagret i url, timestamp.



duplikatsjekk mot siste uke for akkurat det selskapet. en uke holder.

gjør først en duplikatsjekk på linkene i listen du skal søke gjennom;
da slipper du å hente duplikatartikler.

hvordan sjekker man effektivt om to tekster er duplikater?





see if the code runs as it is now. you should get an error. remove the
prediction and 0auth stuff, and get it running without that. it should
be only in main.py. 











get the apple bucket name, get it registered, and train it with an
example.

test it on the same article, and on another article.

get a filter in place that ensures an article will only be stored and
trained once.

make the ui that enables you to train articles manually. show the
text, and have four links: correct|pos|neutral|neg. let the next
article pop up when you click one of these.




make another bucket for google. do the same stuff with that.


get the automation of creating a new bucket in place.

get the access mechanics in place, so each user has his list of picks.

get the buying in place so each user can add to his list.





aaight, so i send all the articles to the same place - the sentiment
is independent of the company, after all. that means i only need to
register one model with google. the form is open in chrome already.

you really should automate genarating a new predictor for each
company, though. sigh. and you should use google, since you want their
smooth scalability. crm would be your own thing, that you would have
to tweak and incorporate and control, i guess.

make the first one apple-specific, and write down every step of the
way to making it work. that way, you know what you have to automate
for the next ones.




i sh-scriptene brukes en google key i stedet for 'language', etc.



no model found. model must first be trained. feil navn på 'language',
da, antakelig.

du må altså finne riktig navn i url'en:

https://www.googleapis.com/prediction/v1.4/trainedmodels/Language%20Identifier/predict?alt=json

Language Identifier, Language Detection, languages, smsspam, languageidentifier og language funker ikke.

etter det kan du bytte den ut med din egen, som du kan kalle news
sentiment e.l. den må trenes før du kan querye den. finn ut hvordan
man gjør det.




det er en error i try credentials, men jeg kan ikke se den, fordi den
printes ikke.

se i predict.py om du har noenlunde samme kall og logikk og gang.

du har byttet ut 'none' med err_str, men det hjalp ikke. ingenting
printes nå.




du må putte tingene fra prediction api inn i templaten, og kalle
templaten etter at du har gjort de tingene.













du får ingen errors, men du får bare displayet 'tmh', selv om du ikke
har slettet noe fra db'en. dvs. at db'en forsvinner hver gang du
stopper serveren? hm. nei, ikke bare kutte serveren. hva da? mye
forandring av kode? tviler litt på det. ny db pga. autorisering, etc?





ok. login required løst ved å importere login_required. det er andre
ting som også må importeres, f.eks. memcache, og kanskje
flowsomething.

nå får du 'tuple not callable' på ('/auth_return',AuthHandler)





try-pred bruker python 27, så det er ikke der problemet ligger.


det funket ikke å inkludere authhandler. hm. det er i alle fall noe
muffens med webapp vs. webapp2. authhandler bruker webapp, og i følge
gae så er det 'no module named webapp'.



name 'login_required' is not defined. sjekk staving, hvor definert, og
hvordan kalt.




auth_return defineres i try-predictions MAIN.PY, som forteller hvilke
klasser som skal handle hvilke url'er. det er authhandler, som nå
ligger nederst i min main.py, som handler auth_return-url'en.

dvs. at så lenge jeg får koblet authhandler på auth-return, så er vi i
boks.







du må ha mange predictors; en for hvert selskap. sjekk pricing.

du må kanskje forandre tittelen til transfarmr? i hvert fall før du
publiserer på nettet. gjør det, og test selv, deretter kan du finne på
et annet navn etterhvert. bruk transfarmr som testbed.



hvis ikke det under funker: kopier try-prediction-scriptet nøyaktig,
og kjør det akkurat slik som beskrevet. se om det funker da, og ekstrapoler.



du trenger antakelig authhandler, som er nederst i home.py. putt den i
main.py, og sjekk at den blir kalt på en eller annen måte.

du må finne ut hva '/auth_return' er for noe, hvor den er definert. se
under: google tror at jeg vil bruke den, i alle fall. den defineres i
if not credentials-seksjonen i main.py. callback = ...



på nettsiden til google, som du havner på når du forsøker på
oauth2client, står det at redirect uri'en er
localhost:8080/auth_return, som du kanskje ikke har definert. har det
noe med dette å gjøre?







i tillegg: hva om scope er for stort? nå ber du jo om en read-write
greie som du ikke har tilgang til, i tillegg til prediction.










du må få definert redirect_url et eller annet sted. det er muligens
det som gjør at auth requesten til google feiler med invalid
client. eller kanskje du har skrevet inn id og secret feil?



du holder på å lese basic steps, på
https://developers.google.com/accounts/docs/OAuth2#scenarios

der vil du få vite hvordan du skal skaffe deg et token.





kanskje du må publishe appen på appengine for at prediction skal
virke? den virker vel neppe lokalt, med mindre du har logget inn som
thomas@dictatr.com, som du kanskje kan gjøre dersom du kommenterer inn
igjen seksjonen om flow i main.py.

-fiks oauth2client credentials

-legg in prediction api. dersom de har fail categories, la neutral
 være det. hvis ikke, la neutral være en egen kategori.

-tren noen artikler.

-se om sentiment classification hjelper deg på noen måte. siden den
 kan bli tatt vekk når som helst må du ikke bli avhengig av den.

-display conclusion om selskap.

-legg inn telleverk som sjekker at antall artikler som blir trent ikke
 er veldig ulikt. f.eks. maks femti artikler flere før du stopper
 treningen.

-legg inn at man bare får sin brukers selskaper.

-legg inn at å legge til et selskap koster penger (dersom man allerede
 har fem eller flere selskaper på listen sin). å legge til selskap
 (til db'en) har du enkel kode for allerede, bare trekk det ut av
 main, og legg det inn i en funksjon.


-legg inn ambiguous articles (display 3 of fewer).

-pynt på displayen av artikler, selskaper. hele siden.

-la magnus, eric, erik evensen, erik aarebrot, thor henning, eskil,
 geir, raymond alfa-teste. fortell dem at de kan kjøpe i vilden sky
 (men tenke at det er ekte penger),
 og at jeg vil kansellere alle innbetalingene deres.

-legg websiden ut på reddit og la folk alfateste den.

-lag app. websiden er gratis, appen koster fem euro. (ellers koster
 ting det samme begge steder.)








drit i adm og optimeringsting; f.eks. hvem som har
tilgang, om ting blir duplisert og om teksten er perfekt.



du får ikke news om selskaper på andre børser enn us. ikke europa, og
ikke asia, f.eks. krx, der samsung er.



bruk url'en til hver artikkel til å sjekke duplikasjon. google
"checking for duplicated content gae put()" or something.



why does it fetch 3 copies of the first article? (3 copies of the
first article link.)



main displays companies from db.

crawl does nothing

sites gives me the links from gf

fetch takes a url (link), reads the article and returns the text

clean cleans the text (called by fetch)

must store junk-cleaned text to db. du er her. let main do this. make
sure the chain from main to sites to fetch and back to main is good.

later, retrieve text from db, clean strict and send to crm. main does
this too.

later, store crm's conclusion to db. and main does this.

later, let main display conclusion with company.



fjern litt javascript i replace characters, ved å fjerne alle ord som
inneholder åpenbare tegn på at de er kode. se fetch.py for detaljer.

lag din egen replace characters. se på gyri sin, og din egen fra
doc/scripts/substitute characters.crm.



kanskje ha en refresh knapp på siden, og bare la ting oppdatere seg
når de kommer, og når de refresher. det er enkelt.




så prøver du med innlogging fra news.google.









har fra news.google uten innlogging. 


linker som ikke er til artikler
fører til krasj.


stripp teksten, og bruk regexer for å finne selve artikklen. beware at
teksten (bloomberg) inneholder instruksjoner og notater til
webutviklerne: "bruk cookie, sett cookien til i morgen."













begynn med en ny side (news.google.com), og prøv å kode slik at det
virker der. deretter tar du yahoo finance. yahoo først, siden google
krever login for å få din egen side med finansnyheter. det er sikkert
lett, gitt at google appengine sin login-funksjon funker. da kan jeg
lese fra min google-konto sin nyhetsside, og legge det inn i db'en.

enkle ad hoc-regex-regler som er greie å begynne med (før den
generelle regex-silingen):
if man finner dette, bruk det, hvis ikke, sil med regex:
dagbladet begynner alltid teksten i artikkelen med "(Dagbladet):"
vg begynner med "(VG nett)"
bloomberg ikke.
dn ikke.
yahoo finance ikke.
ofte står det faktisk (Bloomberg), (Reuters), etc., spesielt dersom
storyen er sakset derfra. Du kan se etter disse.

dersom du skal ha med yahoo finance og google news er det lurt å
begynne med dem, siden de er aggregater av andre nyhetssteder. Dvs. at
det er meningsløst å ta dem med sent, når du allerede har de andre på
plass.




hva med å fjerne alle html-tags(med innhold) som IKKE inneholder minst
ett punktum med space bak (altså minst to setninger)? vha regex,
selvfølgelig, ikke ad hoc.










få til å putte artikkeldelene i db'en.





du har nå lagt inn logging statements i fetch.py som skriver til
fetch.log. det virker ikke med mindre man starter programmet fra kommandolinjen.

nå har du en greie som samler alle artiklene og printer på skjermen,
men den klarer ikke å trekke ut tittel, brødtekst osv. i det hele
tatt.




se på terminaloutputen også når du loader siden (urlfetch bla bla, etc.)









last opp workspace/market_analyzer/ til git.





prøv å bruke en deferred eller asynchronous kommando til å kjøre
nåværende vb.py fra web appen.

så lager du en enkel crawler som kan gi url'er til ovenstående.

så kan du lage en lagrer, som lagrer de innsamlede artiklene (uten
ordentlig siling, ja - du kan jo lagre hver tiende eller noe slikt).





i utgangspunktet funker det å slå sammen vg.py og db.py (i
vb.py). dagbladets titler blir uansett ikke samlet inn for tiden.

et senere steg blir å samle alle selektorene i ett script, og prøve det på
både de norske, og forskjellige internasjonale nettsider.


kan se ut som om shell-appen i eksemplene er en måte å kjøre ting på i
bakgrunnen, uavhengig av brukerne. goodie good!

se også på asynchronous requests (url fetching) du har et vindu åpent
på det.

også background work with the deferred library. vindu åpent på det.

e24 og kapital og de andre pengeavisene (finansavisen, dine penger,
dn.no) har rss feeds.







sjekk hvilke news sites som har api for innsamling av artikler.

sjekk yahoo finance, google news, nyse news, london stock exchange,
forbes, bloomberg, kapital, wall street journal, nyt, techcrunch,
nasdaq, reddit, digg, hacker news, ikke 4chan ennå. hvor har du
skrevet mer om dette? det er flere et sted.

yahoo finance har. nyse også.

twitter har. det finnes også en python-twitter pakke for ubuntu.

nyt har, for bruker-generert innhold. kan sikkert hackes til å virke
for artikler også.

snakk med magnus om opinion-børser, hva de heter, hvordan de
fungerer...



husk at det er data i /home/tmh/django-something i tillegg til det som
er lastet opp til git. disse kan du bruke som grunnlag til å lage nye
admin- og app-templates.

gå gjennom den blå notatboken, samt todo-listen i chrome, todo-listen
på telefonen, 

-lag script som går gjennom prosedyren for å legge til et
 selskap. søketermene manuelt, resten bør være automatisk. først
 prosedyre, på sikt script.

simon i frankrike, magnus, eskil bør være blant alfa-testerne.

chs-filene boostes etter en stunds trening. dette må automatiseres.




DONE:

sjekk status på teab din maskin vs. teab lap og teab på
git. synkroniser, slik at din maskin er oppdatert. done. oppdatert

snakk med eskil. fortell ham at du kutter teab, men starter et
spin-off prosjekt alene. done.

sjekk om lynx og w3m > fil blir bedre enn vb.py. det ble det ikke -
alle linker og denslags er med, uten infoen man får fra
taggene. vanskeligere å parse.

problemet ligger i vb.py. den forårsaker 'illegal seek'. kommenter ut
linjer til du finner hvor feilen ligger. se gjerne gjennom filen
først, for a se om du ser noe åpenbart.

det er bugs i get_Atext, som gjør at teksten fra db ikke kommer med,
og vg gir error - men drit i det. neste steg er som nevnt under å lage
en crawler som samler url'er, og leverer dem til main, som leverer dem
til fetch.py

kan se ut som om du bare trenger en liten kodesnutt som ligner på den
vi brukte i teab til å følge linker. du følger alle hrefs på en side,
og ser hva som skjer. dersom du starter på førstesiden vil du vel til
slutt ha tatt alle linkene på domenet.

se også på hva knut skriver om wget-scriptet sitt.

du har skrevet et script som kjører rdiff-backup, og sender deg en
mail hvis det feiler. det virker antakelig. kjør en gang til og sjekk
at mailen sendes. legg scriptet inn i crontab, både til /media/bck og
til bbg@hjemme.

kjør keygen og ssh copy id begge veier. tri har ekstern ip adresse som
hjemme kan benytte seg av.







du holder på med backups.
finn ut hvordan du gjør rdiff-backup til bbg@hjemme. passord og
slikt. skriv det ned i doc/vikt/rdiff-backup

gjør /bck på hjemme
eid av bbg, og /media/bck eid av tmh.

du fjerner ting fra /bck, og putter dem i /media/bck.

gjør en rdiff-bck fra /home/tmh/ til /media/bck/tmh/ og en fra
/home/bbg/ til /bck/bbg først.

sett locale globalt. det er mulig det hjelper at hele maskinen er klar
for utf-8.

sørg også for å encode til utf-8 før du sender til replace osv.

du har en u på begynnelsen av href-url'ene. kanskje det er den som
lager error?

kanskje den blir fjernet av replace_characters? det er i
alle fall der den burde ligge, tror jeg. encode rett før du sender til
replace, og rett etter at du får ting tilbake derfra.

jeg lurer på om grunnen til at alle gyris scripts har sluttet å virke
er at jeg bruker python 2.5.

i så fall gjør jeg følgende: les på hvor mature python 2.7 er for
google appengine. dersom den er ok, bruk den. hvis ikke, skriv om
beautifulsoup-koden slik at den virker for python 2.5.

bs4 har soup.get_text()!!!!! dersom den virker noenlunde bra er det
himmelsk!

det virker som om python2.7 funker i appengine naa. dessuten blir du
raadet til aa bruke django heller enn webapp templates, which is A
Good Thing.



nå får du tak i brødteksten fra dagbladet vha. editableBodyText
(get_text() er kanskje ikke særlig nyttig likevel). ok,
men teksten forsvinner i løpet av behandlingen av teksten. drit i det,
fokuser på google news med innlogging.

jeg kjører nå python 2.7 og bs4. sjekk om fetch.py funker. putt
deretter inn get_text() og hent news.google.com (i fetch.py) med login
fra mechanize (i crawl.py).

dev_appserver.py får ikke tak i logging-modulen. det er muligens
fordi du har gått over til python 2.7 og ennå ikke har skrevet om
main.py for å reflektere dette. 

drit i github for now. du har lokal git, og du har backup. det holder.

du må sende en post request ved navn 'comp' ('sign') til Companies
handleren for å få lagret ting. dette kan antakelig gjøres enklere
siden du ikke trenger å handle et knappeklikk og tekst i en inputboks.

få main til å vise company name og ticker uten noe annet dill. 

lagre til, og hente fra, db.

du holder på med regex for setninger. du har funnet et par stykker, og
den siste ser ut til å funke på dem som den første ikke funket
på. finn et par til og se.




google.com/finance har relevante nyheter for hver selskap! bruk
det. finn 'a', finn 'lead_story_url:', strengene som kommer etter
det er linker til relevante stories, hi hi.


correct the display of 'user'. this was due to the two debug entries
in fetch.py, where you fetched and printed text from block.py


you have good enough texts. making them better is optimizing.

that way, when they want to read the article, you first show them the
text you put into crm with the link, and if they want to read that,
they click the link. perhaps you can even track that, and look at the
articles for which they click the link, so you can see why they
weren't happy just reading the text for crm.

prøv om wrapperen til crm114 faktisk funker. antakelig ikke, men vær
sikker før du antar!

finn crm.py, laget av openvest development. putt den i folderen, og
rename din egen crm.py til noe annet. se om det da går an å bruke
den. (dersom openvest sin crm.py inneholder subprocess modulen, så vil
det ikke gå. da må du finne på noe annet.







du har problemer med subprocess-modulen, som tilsynelatende ikke har
en call() funksjon, selv om den skal ha det. subprocesses not allowed
in google appengine. d'oh.

how to start crm114 then? some stupid wrap?





plugg inn crm nå. 
bygg opp databasen til to selskaper. ikke mer. nå er databasen
tom. done.



du bruker google sin prediction api. den kommer ikke til å koste deg
noe med mindre dette går strålende, så det er greit. sjekk om det
allerede finnes trenede good_business_news | bad_business_news om
spesifikke selskaper, eller generelt om business.


du får printet ting mellom hver a. ha ha, du har fikset det. content =
articles.content osv. 





make sure the articles are stored to the db. delete everything in db,
and start over.

store the articles to db with their respective companies and
datetimes. urls as well. 

ok. nå spør den brukeren om permissions, som er fremskritt, men når
den skal redirecte til auth_return, så får jeg 404. dvs. at
auth_return er definert i f.eks. javascript.











hva gjorde du den dagen?:
-many to many mellom user og companies. apparently ok.
-unsubscribe button. ok.
-add company to user AFTER ok, not before. ok.
-goog, ikke googl. ok.

skal gjøres
-fiks ibm (det skal være punktum). er det fordi den er på nyse, og
IKKE på nasdaq? 
-home link in company view. ok.
-fiks slik at det er navn, og ikke keys, som flyttes. hvor var dette?
dette var pairs til index. ok.



is there a cursor now? does it work as it should?
was it solved because there were no companies in the db? don't
remember, but i think it was fixed. at least it went away, but perhaps
because the cursor is no longer there.
it seems the cursor error is because of something other than that line of code
itself. something has caused 'companies' in scrape to be something
else than it used to be. no longer a query result?
suddenly, scrape isn't working anymore. error on cursor. "list object
does not have attribute cursor()."



du skal fikse det slik at linkz() i scrape også tar andre børser enn
nasdaq.
if exchange == nyse:
https://www.google.com/finance/company_news?q=NYSE%3AIBM&ei=cP1zUvCHDdOBwAO5GQ

you need some robustness around whether the name is spelled with or
without the final dot. there seems to be no consensus amoung the
coders at nasdaq. do i? i just scrape the name, after all.

nå også intermittent deadline på scrape lokalt. hm.

intermittent cursor error in scrape. try putting the for company in
companies: process_links(company) AFTER the if len(companies)
statement.




hverken memcache.delete() eller .flush_all() virker, tilsynelatende.

aller først: nå har det gått mer enn 180 minutter siden i går
kveld. prøv /scrape en gang, og se hva som printes i listen som heter c.

if cursor remove does not work, or if you keep having
deadlineexceederrors, have a look at the deferred article you have
opened at
https://developers.google.com/appengine/articles/deferred?csw=1

cursoren blir ikke reset til begynnelsen noe sted. du har et spørsmål
ute på stack.



nå printes selskapene du har prosessert i scrape rett før "you have
scraped...". sjekk om en stund at denne listen faktisk inneholder
noe. the companies now have 44, 96, 57, 69 and 45 articles. they
should have more when you run scrape again in a while.

deretter remove c from scrape and main.




deadline exceeded online. put cursor in analyze and clean as well, and
perhaps fewer companies/articles per iteration.



ok. so now, thins work without errors, but you are not cleaning and
analyzing older articles. you need something that catches those as
well, in case they fall through the hoops of memcached recent
articles.

just run a regular, old style clean and analyze, with or without
cursor, to get those old ones.


how long are article ids stored in memcache? can you use only the id
list to find dupes? if you store it long enough, and don't delete ids
from that list, you should be alright.




in dupes, you should only fetch articles from the ids in memcache. no
need to check all articles. hm. but you need to compare to
something. you can compare to the articles from a few hours back.


it's because dupes removes some of the ones scrape puts in - those are
the ones that are none objects! d'uh.

let dupes fetch the ids from memcache, and remove them as it removes
their counterpart articles.



the error comes only when you + the two lists together. both lists
do contains something, so what is the problem?

the ids stored in memcache by scrape causes crash in clean if they
contain previous keys. only new keys => fine. new keys and old keys =>
article is none type object.




two lists: 

done:
one cropped by analyze - the ones to clean and analyze next
time. (added to by scrape, cropped by dupes, not changed by clean, cropped by analyze.)

done:
one NOT cropped by analyze - the one use to check for
duplicates. (added to by scrape, cropped by dupes, not changed by clean or analyze.)






nå er det zombie-id'er i db'en igjen. main støter på det når den
prøver å hente user companies. hvor er det jeg ikke sletter dem
skikkelig? det virket som en engangs, som lå igjen fra forrige
versjons kjøring.

